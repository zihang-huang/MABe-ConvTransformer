{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 59156,
     "databundleVersionId": 13874099,
     "sourceType": "competition"
    },
    {
     "sourceId": 14296230,
     "sourceType": "datasetVersion",
     "datasetId": 9123026
    }
   ],
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "import os\nimport sys\nfrom pathlib import Path\nfrom collections import OrderedDict\nfrom typing import Dict, List, Tuple, Any, Optional\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\n\n# Check if running on Kaggle\nON_KAGGLE = os.path.exists('/kaggle/input')\n\nif ON_KAGGLE:\n    # Kaggle paths\n    DATA_DIR = Path('/kaggle/input/MABe-mouse-behavior-detection')\n    CHECKPOINT_PATH = Path('/kaggle/input/convtransformer/MABe-ConvTransformer/outputs/checkpoints/mabe-epoch=97-val_segment_f1=0.2440.ckpt')\n    OUTPUT_DIR = Path('/kaggle/working')\n    # Add src to path\n    sys.path.insert(0, '/kaggle/input/convtransformer/MABe-ConvTransformer')\n\nprint(f\"Data directory: {DATA_DIR}\")\nprint(f\"Checkpoint path: {CHECKPOINT_PATH}\")\nprint(f\"Output directory: {OUTPUT_DIR}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-26T03:01:19.503597Z",
     "iopub.execute_input": "2025-12-26T03:01:19.504317Z",
     "iopub.status.idle": "2025-12-26T03:01:28.410414Z",
     "shell.execute_reply.started": "2025-12-26T03:01:19.504292Z",
     "shell.execute_reply": "2025-12-26T03:01:28.409815Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Import from src\nfrom src.models.lightning_module import BehaviorRecognitionModule\nfrom src.data.preprocessing import (\n    CoordinateNormalizer,\n    TemporalResampler,\n    MissingDataHandler,\n    BodyPartMapper\n)\nfrom src.utils.postprocessing import (\n    aggregate_window_predictions,\n    extract_segments,\n    merge_segments,\n    apply_nms,\n    create_submission,\n    BehaviorSegment,\n)\n\nprint(\"Imports successful!\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-26T03:01:28.411494Z",
     "iopub.execute_input": "2025-12-26T03:01:28.411781Z",
     "iopub.status.idle": "2025-12-26T03:01:45.228832Z",
     "shell.execute_reply.started": "2025-12-26T03:01:28.411764Z",
     "shell.execute_reply": "2025-12-26T03:01:45.227992Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Configuration",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Model and data configuration\nCONFIG = {\n    'window_size': 512,\n    'stride': 256,\n    'target_fps': 30.0,\n    'batch_size': 64,\n    'num_workers': 2,\n    \n    # Evaluation settings - OPTIMIZED based on validation analysis\n    # Lower threshold to capture more behaviors (was 0.39, best macro_f1 at 0.30)\n    'threshold': 0.30,\n    'min_duration': 5,\n    'smoothing_kernel': 5,\n    'nms_threshold': 0.3,\n    'merge_gap': 5,\n    \n    # Per-behavior thresholds for rare classes\n    # (submit has max prob ~0.05, so we need special handling)\n    'behavior_thresholds': {\n        'submit': 0.02,  # Very rare, need lower threshold\n        'chaseattack': 0.25,  # Rare but detectable\n    },\n}\n\n# Behavior classes (must match training)\nBEHAVIORS = [\n    # Self behaviors\n    'biteobject', 'climb', 'dig', 'exploreobject', 'freeze',\n    'genitalgroom', 'huddle', 'rear', 'rest', 'run', 'selfgroom',\n    # Pair behaviors\n    'allogroom', 'approach', 'attack', 'attemptmount', 'avoid',\n    'chase', 'chaseattack', 'defend', 'disengage', 'dominance',\n    'dominancegroom', 'dominancemount', 'ejaculate', 'escape',\n    'flinch', 'follow', 'intromit', 'mount', 'reciprocalsniff',\n    'shepherd', 'sniff', 'sniffbody', 'sniffface', 'sniffgenital',\n    'submit', 'tussle'\n]\n\n# Test behaviors (what Kaggle evaluates)\nTEST_BEHAVIORS = ['approach', 'attack', 'avoid', 'chase', 'chaseattack', 'submit', 'rear']\n\nprint(f\"Number of behavior classes: {len(BEHAVIORS)}\")\nprint(f\"Test behaviors: {TEST_BEHAVIORS}\")\nprint(f\"Default threshold: {CONFIG['threshold']}\")\nprint(f\"Per-behavior thresholds: {CONFIG['behavior_thresholds']}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-26T03:01:45.229637Z",
     "iopub.execute_input": "2025-12-26T03:01:45.230103Z",
     "iopub.status.idle": "2025-12-26T03:01:45.235686Z",
     "shell.execute_reply.started": "2025-12-26T03:01:45.230082Z",
     "shell.execute_reply": "2025-12-26T03:01:45.235003Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Datasets",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class TestDataset(Dataset):\n    \"\"\"\n    Dataset for loading test tracking data (no annotations).\n    Generates sliding windows over all agent-target pairs.\n    \"\"\"\n    \n    def __init__(\n        self,\n        metadata_df: pd.DataFrame,\n        tracking_dir: Path,\n        behaviors: List[str],\n        window_size: int = 512,\n        stride: int = 256,\n        target_fps: float = 30.0,\n        tracking_cache_size: int = 4\n    ):\n        self.tracking_dir = Path(tracking_dir)\n        self.window_size = window_size\n        self.stride = stride\n        self.target_fps = target_fps\n        self.tracking_cache_size = max(1, tracking_cache_size)\n        \n        self.behaviors = behaviors\n        self.num_classes = len(behaviors)\n        \n        # Preprocessors\n        self.coord_normalizer = CoordinateNormalizer()\n        self.temporal_resampler = TemporalResampler(target_fps)\n        self.missing_handler = MissingDataHandler()\n        self.bodypart_mapper = BodyPartMapper(use_core_only=False)\n        \n        # Cache\n        self._tracking_cache: OrderedDict = OrderedDict()\n        \n        # Parse metadata\n        self.metadata_df = metadata_df.copy()\n        self.video_ids = metadata_df['video_id'].unique().tolist()\n        \n        # Build sample index\n        self.samples = self._build_sample_index()\n        print(f\"Built {len(self.samples)} test samples from {len(self.video_ids)} videos\")\n    \n    def _get_fps(self, metadata: Dict) -> float:\n        return metadata.get('frames_per_second',\n                           metadata.get('frames per second',\n                                        metadata.get('fps', 30)))\n    \n    def _build_sample_index(self) -> List[Dict]:\n        samples = []\n        \n        for _, row in tqdm(self.metadata_df.iterrows(), total=len(self.metadata_df), desc=\"Building sample index\"):\n            lab_id = row['lab_id']\n            video_id = row['video_id']\n            fps = self._get_fps(row)\n            \n            # Load tracking to get video length and mice\n            track_path = self.tracking_dir / f\"{lab_id}\" / f\"{video_id}.parquet\"\n            if not track_path.exists():\n                continue\n            \n            track_df = pd.read_parquet(track_path)\n            n_frames = track_df['video_frame'].max() + 1\n            mice = sorted(track_df['mouse_id'].unique())\n            \n            # Adjust for resampling\n            if fps != self.target_fps:\n                duration = n_frames / fps\n                n_frames = int(duration * self.target_fps)\n            \n            # Generate windows for each agent-target pair\n            for start in range(0, max(1, n_frames - self.window_size + 1), self.stride):\n                for agent in mice:\n                    for target in mice:\n                        samples.append({\n                            'lab_id': lab_id,\n                            'video_id': video_id,\n                            'start_frame': start,\n                            'agent_id': agent,\n                            'target_id': target,\n                            'metadata': row.to_dict()\n                        })\n        \n        return samples\n    \n    def __len__(self) -> int:\n        return len(self.samples)\n    \n    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n        sample_info = self.samples[idx]\n        features, valid_mask = self._load_tracking(sample_info)\n        \n        # Dummy labels (zeros) for test data\n        labels = np.zeros((self.window_size, self.num_classes), dtype=np.float32)\n        \n        return {\n            'features': torch.from_numpy(features),\n            'labels': torch.from_numpy(labels),\n            'valid_mask': torch.from_numpy(valid_mask),\n            'video_id': sample_info['video_id'],\n            'agent_id': sample_info['agent_id'],\n            'target_id': sample_info['target_id'],\n            'start_frame': sample_info['start_frame']\n        }\n    \n    def _get_cached_tracking(self, lab_id: str, video_id: str, metadata: Dict):\n        key = (lab_id, video_id)\n        if key in self._tracking_cache:\n            self._tracking_cache.move_to_end(key)\n            return self._tracking_cache[key]\n        \n        track_path = self.tracking_dir / f\"{lab_id}\" / f\"{video_id}.parquet\"\n        if not track_path.exists():\n            raise FileNotFoundError(f\"Tracking file not found: {track_path}\")\n        \n        track_df = pd.read_parquet(track_path)\n        fps = self._get_fps(metadata)\n        bodyparts = sorted(track_df['bodypart'].unique().tolist())\n        \n        coords_by_mouse = {}\n        valid_by_mouse = {}\n        \n        for mouse_id in track_df['mouse_id'].unique():\n            raw_coords = self._extract_mouse_coords(track_df, mouse_id, bodyparts)\n            mapped_coords, mapped_parts, availability = self.bodypart_mapper.map_bodyparts(raw_coords, bodyparts)\n            mapped_coords = self.bodypart_mapper.compute_derived_parts(mapped_coords, mapped_parts, availability)\n            \n            if fps != self.target_fps:\n                mapped_coords = self.temporal_resampler(mapped_coords, fps)\n            \n            mapped_coords = self.coord_normalizer(mapped_coords, metadata)\n            mapped_coords, valid_mask = self.missing_handler.interpolate_missing(mapped_coords)\n            mapped_coords = np.nan_to_num(mapped_coords, nan=0.0)\n            \n            coords_by_mouse[mouse_id] = mapped_coords.astype(np.float32)\n            valid_by_mouse[mouse_id] = valid_mask.astype(np.float32)\n        \n        cache_entry = {\n            'coords_by_mouse': coords_by_mouse,\n            'valid_by_mouse': valid_by_mouse\n        }\n        self._tracking_cache[key] = cache_entry\n        \n        if len(self._tracking_cache) > self.tracking_cache_size:\n            self._tracking_cache.popitem(last=False)\n        \n        return cache_entry\n    \n    def _extract_mouse_coords(self, track_df: pd.DataFrame, mouse_id: int, bodyparts: List[str]) -> Dict[str, np.ndarray]:\n        mouse_df = track_df[track_df['mouse_id'] == mouse_id].copy()\n        n_frames = track_df['video_frame'].max() + 1\n        \n        coords = {}\n        for bp in bodyparts:\n            bp_df = mouse_df[mouse_df['bodypart'] == bp].sort_values('video_frame')\n            frames = bp_df['video_frame'].values\n            part_coords = np.full((n_frames, 2), np.nan, dtype=np.float32)\n            part_coords[frames, 0] = bp_df['x'].values\n            part_coords[frames, 1] = bp_df['y'].values\n            coords[bp] = part_coords\n        \n        return coords\n    \n    def _load_tracking(self, sample_info: Dict) -> Tuple[np.ndarray, np.ndarray]:\n        lab_id = sample_info['lab_id']\n        video_id = sample_info['video_id']\n        start_frame = sample_info['start_frame']\n        agent_id = sample_info['agent_id']\n        target_id = sample_info['target_id']\n        metadata = sample_info['metadata']\n        \n        cache_entry = self._get_cached_tracking(lab_id, video_id, metadata)\n        coords_by_mouse = cache_entry['coords_by_mouse']\n        valid_by_mouse = cache_entry['valid_by_mouse']\n        \n        agent_coords = coords_by_mouse.get(agent_id)\n        target_coords = coords_by_mouse.get(target_id)\n        agent_valid = valid_by_mouse.get(agent_id)\n        target_valid = valid_by_mouse.get(target_id)\n        \n        if agent_coords is None or target_coords is None:\n            raise ValueError(f\"Missing coordinates for agent {agent_id} or target {target_id} in {video_id}\")\n        \n        end_frame = start_frame + self.window_size\n        agent_window = self._get_window(agent_coords, start_frame, end_frame)\n        target_window = self._get_window(target_coords, start_frame, end_frame)\n        \n        features = np.concatenate([\n            agent_window.reshape(self.window_size, -1),\n            target_window.reshape(self.window_size, -1)\n        ], axis=-1)\n        \n        agent_valid_window = self._get_window(agent_valid.astype(np.float32), start_frame, end_frame)\n        target_valid_window = self._get_window(target_valid.astype(np.float32), start_frame, end_frame)\n        valid_mask = (agent_valid_window.mean(axis=-1) > 0.5) & (target_valid_window.mean(axis=-1) > 0.5)\n        \n        return features.astype(np.float32), valid_mask.astype(np.float32)\n    \n    def _get_window(self, data: np.ndarray, start: int, end: int) -> np.ndarray:\n        n_frames = data.shape[0]\n        \n        if start < 0:\n            pre_pad = -start\n            start = 0\n        else:\n            pre_pad = 0\n        \n        if end > n_frames:\n            post_pad = end - n_frames\n            end = n_frames\n        else:\n            post_pad = 0\n        \n        window = data[start:end]\n        \n        if pre_pad > 0 or post_pad > 0:\n            pad_width = [(pre_pad, post_pad)] + [(0, 0)] * (window.ndim - 1)\n            window = np.pad(window, pad_width, mode='edge')\n        \n        return window",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-26T03:01:45.236537Z",
     "iopub.execute_input": "2025-12-26T03:01:45.237253Z",
     "iopub.status.idle": "2025-12-26T03:01:45.270660Z",
     "shell.execute_reply.started": "2025-12-26T03:01:45.237228Z",
     "shell.execute_reply": "2025-12-26T03:01:45.269941Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load checkpoint with safe globals for numpy scalars\nsafe_classes = [np.core.multiarray.scalar]\ntry:\n    torch.serialization.add_safe_globals(safe_classes)\nexcept Exception:\n    pass\n\ntry:\n    safe_ctx = torch.serialization.safe_globals(safe_classes)\nexcept Exception:\n    from contextlib import nullcontext\n    safe_ctx = nullcontext()\n\nprint(f\"Loading model from {CHECKPOINT_PATH}\")\nwith safe_ctx:\n    model = BehaviorRecognitionModule.load_from_checkpoint(\n        str(CHECKPOINT_PATH),\n        map_location=device,\n        weights_only=False,\n        strict=True,\n    )\n\nmodel.to(device)\nmodel.eval()\n\nprint(f\"Model loaded successfully!\")\nprint(f\"  - Model type: {model.model_name}\")\nprint(f\"  - Input dim: {model.input_dim}\")\nprint(f\"  - Num classes: {model.num_classes}\")\nprint(f\"  - Behaviors: {model.behaviors[:5]}... ({len(model.behaviors)} total)\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-26T03:01:45.272627Z",
     "iopub.execute_input": "2025-12-26T03:01:45.272876Z",
     "iopub.status.idle": "2025-12-26T03:01:46.020100Z",
     "shell.execute_reply.started": "2025-12-26T03:01:45.272860Z",
     "shell.execute_reply": "2025-12-26T03:01:46.019389Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Inference",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load test metadata\ntest_csv = DATA_DIR / 'test.csv'\ntest_df = pd.read_csv(test_csv)\nprint(f\"Test metadata: {len(test_df)} videos\")\nprint(test_df.head())\n\n# Create test dataset\ntest_dataset = TestDataset(\n    metadata_df=test_df,\n    tracking_dir=DATA_DIR / 'test_tracking',\n    behaviors=BEHAVIORS,\n    window_size=CONFIG['window_size'],\n    stride=CONFIG['stride'],\n    target_fps=CONFIG['target_fps'],\n    tracking_cache_size=8\n)\n\nprint(f\"\\nTest dataset: {len(test_dataset)} samples\")\n\n# Create dataloader\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=CONFIG['batch_size'],\n    shuffle=False,\n    num_workers=CONFIG['num_workers'],\n    pin_memory=True,\n    drop_last=False\n)\n\ndef _to_scalar(value: Any) -> Any:\n    \"\"\"Convert tensors/NumPy scalars to plain Python values.\"\"\"\n    if isinstance(value, torch.Tensor):\n        value = value.item()\n    if isinstance(value, np.generic):\n        value = value.item()\n    return value\n\n\ndef _format_mouse_id(mouse_id: Any, agent_id: Any = None) -> str:\n    \"\"\"\n    Normalize mouse identifiers to submission format.\n    \n    Args:\n        mouse_id: The mouse ID to format (already 1-indexed from tracking data)\n        agent_id: If provided and equals mouse_id, returns 'self' (for target formatting)\n    \n    Returns:\n        Formatted string: 'mouse1', 'mouse2', etc. or 'self' if target == agent\n    \"\"\"\n    mouse_id = _to_scalar(mouse_id)\n    agent_id = _to_scalar(agent_id) if agent_id is not None else None\n    \n    # Check if this is a self-reference (target == agent)\n    if agent_id is not None and mouse_id == agent_id:\n        return 'self'\n    \n    # Handle string mouse IDs\n    if isinstance(mouse_id, str):\n        cleaned = mouse_id.strip()\n        if cleaned.lower().startswith('mouse'):\n            return cleaned\n        if cleaned.lstrip('-').isdigit():\n            mouse_id = int(cleaned)\n        else:\n            return cleaned\n    \n    # Convert to integer and format as mouse ID\n    try:\n        mouse_int = int(mouse_id)\n    except (TypeError, ValueError):\n        return str(mouse_id)\n    \n    # Tracking data uses 1-indexed mouse IDs (1, 2, 3, 4)\n    # Format directly as 'mouse1', 'mouse2', etc. to match behaviors_labeled\n    return f'mouse{mouse_int}'",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-26T03:01:46.020970Z",
     "iopub.execute_input": "2025-12-26T03:01:46.021248Z",
     "iopub.status.idle": "2025-12-26T03:01:46.547866Z",
     "shell.execute_reply.started": "2025-12-26T03:01:46.021226Z",
     "shell.execute_reply": "2025-12-26T03:01:46.547087Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def collect_predictions(\n    model: BehaviorRecognitionModule,\n    dataloader: DataLoader,\n    device: torch.device\n) -> List[Dict]:\n    \"\"\"\n    Run model on dataloader and collect window-level predictions.\n    \"\"\"\n    window_predictions = []\n    \n    model.eval()\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Running inference\"):\n            features = batch['features'].to(device)\n            mask = batch.get('valid_mask')\n            if mask is not None:\n                mask = mask.to(device)\n            \n            logits = model(features, mask)\n            probs = torch.sigmoid(logits)\n            \n            for i in range(features.shape[0]):\n                window_prob = probs[i].detach().cpu().numpy()\n                if mask is not None:\n                    window_mask = mask[i].detach().cpu().numpy().reshape(-1, 1)\n                    window_prob = window_prob * window_mask\n                \n                window_predictions.append({\n                    'video_id': _to_scalar(batch['video_id'][i]),\n                    'agent_id': _to_scalar(batch['agent_id'][i]),\n                    'target_id': _to_scalar(batch['target_id'][i]),\n                    'start_frame': int(_to_scalar(batch['start_frame'][i])),\n                    'probabilities': window_prob,\n                })\n    \n    return window_predictions\n\n\n# Run inference\nprint(\"Running inference on test data...\")\nwindow_predictions = collect_predictions(model, test_loader, device)\nprint(f\"Collected {len(window_predictions)} window predictions\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-26T03:01:46.548868Z",
     "iopub.execute_input": "2025-12-26T03:01:46.549167Z",
     "iopub.status.idle": "2025-12-26T03:01:52.975990Z",
     "shell.execute_reply.started": "2025-12-26T03:01:46.549141Z",
     "shell.execute_reply": "2025-12-26T03:01:52.974937Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Post Processing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def resolve_overlaps(segments: List[BehaviorSegment], min_duration: int = 5) -> List[BehaviorSegment]:\n    \"\"\"\n    Resolve overlapping behaviors for the same agent-target pair.\n    \n    When two behaviors overlap for the same (video_id, agent_id, target_id),\n    the latter behavior's start_frame is adjusted to begin right after the\n    previous behavior's stop_frame.\n    \n    Args:\n        segments: List of BehaviorSegment objects\n        min_duration: Minimum duration for a valid segment\n    \n    Returns:\n        List of BehaviorSegment objects with overlaps resolved\n    \"\"\"\n    from collections import defaultdict\n    \n    # Group segments by (video_id, agent_id, target_id)\n    groups = defaultdict(list)\n    for seg in segments:\n        key = (seg.video_id, seg.agent_id, seg.target_id)\n        groups[key].append(seg)\n    \n    resolved_segments = []\n    \n    for key, group_segments in groups.items():\n        # Sort by start_frame, then by stop_frame (to handle ties)\n        group_segments.sort(key=lambda s: (s.start_frame, s.stop_frame))\n        \n        # Track the end of the last non-overlapping segment\n        last_end = -1\n        \n        for seg in group_segments:\n            new_start = seg.start_frame\n            new_stop = seg.stop_frame\n            \n            # If this segment overlaps with the previous one, adjust start\n            if new_start < last_end:\n                new_start = last_end\n            \n            # Check if segment is still valid after adjustment\n            if new_start < new_stop and (new_stop - new_start) >= min_duration:\n                # Create a new segment with adjusted start\n                resolved_seg = BehaviorSegment(\n                    video_id=seg.video_id,\n                    agent_id=seg.agent_id,\n                    target_id=seg.target_id,\n                    action=seg.action,\n                    start_frame=new_start,\n                    stop_frame=new_stop,\n                    confidence=seg.confidence,\n                )\n                resolved_segments.append(resolved_seg)\n                last_end = new_stop\n            # If segment becomes invalid (too short or start >= stop), skip it\n    \n    return resolved_segments\n\n\ndef extract_segments_with_per_behavior_threshold(\n    frame_probs: np.ndarray,\n    behavior_names: List[str],\n    default_threshold: float = 0.3,\n    behavior_thresholds: Dict[str, float] = None,\n    min_duration: int = 5,\n    smoothing_kernel: int = 5\n) -> List[Tuple[str, int, int, float]]:\n    \"\"\"\n    Convert frame-level probabilities to behavior segments with per-behavior thresholds.\n    \n    Args:\n        frame_probs: Array of shape (n_frames, n_behaviors)\n        behavior_names: List of behavior class names\n        default_threshold: Default probability threshold\n        behavior_thresholds: Dict mapping behavior names to custom thresholds\n        min_duration: Minimum segment duration in frames\n        smoothing_kernel: Size of median filter for smoothing\n    \n    Returns:\n        List of (behavior, start_frame, stop_frame, confidence) tuples\n    \"\"\"\n    from scipy.ndimage import median_filter\n    \n    n_frames, n_behaviors = frame_probs.shape\n    segments = []\n    behavior_thresholds = behavior_thresholds or {}\n    \n    for behavior_idx in range(n_behaviors):\n        probs = frame_probs[:, behavior_idx].copy()\n        behavior = behavior_names[behavior_idx]\n        \n        # Get threshold for this behavior\n        threshold = behavior_thresholds.get(behavior, default_threshold)\n        \n        # Apply temporal smoothing\n        if smoothing_kernel > 1:\n            probs = median_filter(probs, size=smoothing_kernel)\n        \n        # Threshold to binary\n        binary = (probs >= threshold).astype(np.int32)\n        \n        # Find contiguous regions\n        diff = np.diff(np.concatenate([[0], binary, [0]]))\n        starts = np.where(diff == 1)[0]\n        ends = np.where(diff == -1)[0]\n        \n        # Filter by duration and add confidence\n        for start, end in zip(starts, ends):\n            duration = end - start\n            if duration >= min_duration:\n                confidence = float(probs[start:end].mean())\n                segments.append((behavior, start, end, confidence))\n    \n    return segments\n\n\ndef build_submission_rows(\n    window_predictions: List[Dict],\n    behaviors: List[str],\n    threshold: float,\n    min_duration: int,\n    smoothing_kernel: int,\n    nms_threshold: float,\n    merge_gap: int = 5,\n    behavior_thresholds: Dict[str, float] = None,\n) -> List[Dict]:\n    \"\"\"\n    Convert window-level predictions into submission-format rows.\n    \n    Now supports per-behavior thresholds for rare classes.\n    \"\"\"\n    print(\"Aggregating window predictions...\")\n    aggregated = aggregate_window_predictions(window_predictions, overlap_strategy='average')\n    print(f\"  {len(aggregated)} unique (video, agent, target) combinations\")\n    \n    all_segments: List[BehaviorSegment] = []\n    \n    print(\"Extracting segments...\")\n    for (video_id, agent_id, target_id), frame_probs in tqdm(aggregated.items(), desc=\"Processing videos\"):\n        # Use per-behavior thresholds\n        raw_segments = extract_segments_with_per_behavior_threshold(\n            frame_probs,\n            behaviors,\n            default_threshold=threshold,\n            behavior_thresholds=behavior_thresholds,\n            min_duration=min_duration,\n            smoothing_kernel=smoothing_kernel,\n        )\n        merged = merge_segments(raw_segments, gap_threshold=merge_gap)\n        final_segments = apply_nms(merged, iou_threshold=nms_threshold)\n        \n        # Format mouse IDs: agent as \"mouse1\", \"mouse2\", etc.\n        # target as \"self\" if same as agent, otherwise \"mouse1\", \"mouse2\", etc.\n        formatted_agent = _format_mouse_id(agent_id)\n        formatted_target = _format_mouse_id(target_id, agent_id=agent_id)\n        \n        for behavior, start, stop, conf in final_segments:\n            all_segments.append(BehaviorSegment(\n                video_id=int(_to_scalar(video_id)),\n                agent_id=formatted_agent,\n                target_id=formatted_target,\n                action=behavior,\n                start_frame=int(start),\n                stop_frame=int(stop),\n                confidence=float(conf),\n            ))\n    \n    # Resolve overlapping behaviors for the same agent-target pair\n    print(\"Resolving overlapping behaviors...\")\n    all_segments = resolve_overlaps(all_segments, min_duration=min_duration)\n    print(f\"  {len(all_segments)} segments after overlap resolution\")\n    \n    # Sort segments and create submission rows directly\n    print(\"Creating submission rows...\")\n    all_segments.sort(key=lambda s: (s.video_id, s.agent_id, s.target_id, s.start_frame))\n    \n    rows = []\n    for row_id, seg in enumerate(all_segments):\n        if seg.duration >= min_duration and seg.start_frame < seg.stop_frame:\n            rows.append({\n                'row_id': row_id,\n                'video_id': seg.video_id,\n                'agent_id': seg.agent_id,\n                'target_id': seg.target_id,\n                'action': seg.action,\n                'start_frame': seg.start_frame,\n                'stop_frame': seg.stop_frame,\n            })\n    \n    return rows\n\n\n# Build submission with per-behavior thresholds\nsubmission_rows = build_submission_rows(\n    window_predictions,\n    BEHAVIORS,\n    threshold=CONFIG['threshold'],\n    min_duration=CONFIG['min_duration'],\n    smoothing_kernel=CONFIG['smoothing_kernel'],\n    nms_threshold=CONFIG['nms_threshold'],\n    merge_gap=CONFIG['merge_gap'],\n    behavior_thresholds=CONFIG.get('behavior_thresholds', {}),\n)\n\nprint(f\"\\nGenerated {len(submission_rows)} submission rows\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-26T03:01:52.977596Z",
     "iopub.execute_input": "2025-12-26T03:01:52.977891Z",
     "iopub.status.idle": "2025-12-26T03:02:03.537844Z",
     "shell.execute_reply.started": "2025-12-26T03:01:52.977852Z",
     "shell.execute_reply": "2025-12-26T03:02:03.536701Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Submission",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create submission DataFrame\nsubmission_df = pd.DataFrame(\n    submission_rows,\n    columns=['row_id', 'video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame']\n)\n\n# Save submission\nsubmission_path = OUTPUT_DIR / 'submission.csv'\nsubmission_df.to_csv(submission_path, index=False)\n\nprint(f\"Submission saved to {submission_path}\")\nprint(f\"\\nSubmission shape: {submission_df.shape}\")\nprint(f\"\\nSubmission preview:\")\nprint(submission_df.head(20))",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-26T03:02:03.538786Z",
     "iopub.execute_input": "2025-12-26T03:02:03.539099Z",
     "iopub.status.idle": "2025-12-26T03:02:03.560230Z",
     "shell.execute_reply.started": "2025-12-26T03:02:03.539072Z",
     "shell.execute_reply": "2025-12-26T03:02:03.559292Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Submission statistics\nprint(\"\\n=== Submission Statistics ===\")\nprint(f\"Total predictions: {len(submission_df)}\")\nprint(f\"Unique videos: {submission_df['video_id'].nunique()}\")\nprint(f\"\\nPredictions per action:\")\nprint(submission_df['action'].value_counts().head(20))\n\nprint(f\"\\nAverage segment duration:\")\nsubmission_df['duration'] = submission_df['stop_frame'] - submission_df['start_frame']\nprint(f\"  Mean: {submission_df['duration'].mean():.1f} frames\")\nprint(f\"  Median: {submission_df['duration'].median():.1f} frames\")\nprint(f\"  Min: {submission_df['duration'].min()} frames\")\nprint(f\"  Max: {submission_df['duration'].max()} frames\")\n\n# Validate test behavior coverage\nprint(\"\\n=== Test Behavior Coverage ===\")\npredicted_behaviors = set(submission_df['action'].unique())\nmissing_test_behaviors = set(TEST_BEHAVIORS) - predicted_behaviors\ncovered_test_behaviors = set(TEST_BEHAVIORS) & predicted_behaviors\n\nprint(f\"Test behaviors covered: {len(covered_test_behaviors)}/{len(TEST_BEHAVIORS)}\")\nfor beh in TEST_BEHAVIORS:\n    count = (submission_df['action'] == beh).sum()\n    status = \"✓\" if beh in covered_test_behaviors else \"✗ MISSING\"\n    print(f\"  {beh:15s}: {count:5d} predictions {status}\")\n\nif missing_test_behaviors:\n    print(f\"\\n⚠️  WARNING: Missing predictions for: {missing_test_behaviors}\")\n    print(\"   Consider lowering per-behavior thresholds for these behaviors\")\nelse:\n    print(\"\\n✓ All test behaviors have predictions\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-26T03:02:03.561430Z",
     "iopub.execute_input": "2025-12-26T03:02:03.561692Z",
     "iopub.status.idle": "2025-12-26T03:02:03.578946Z",
     "shell.execute_reply.started": "2025-12-26T03:02:03.561674Z",
     "shell.execute_reply": "2025-12-26T03:02:03.578393Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Final check\nprint(\"\\n=== Final Checks ===\")\nprint(f\"Submission file exists: {submission_path.exists()}\")\nprint(f\"File size: {submission_path.stat().st_size / 1024:.1f} KB\")\n\n# Verify columns\nexpected_columns = ['row_id', 'video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame']\nactual_columns = list(submission_df.columns[:7])\nprint(f\"\\nColumns match expected: {actual_columns == expected_columns}\")\n\nprint(\"\\nDone!\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-26T03:02:03.579736Z",
     "iopub.execute_input": "2025-12-26T03:02:03.580017Z",
     "iopub.status.idle": "2025-12-26T03:02:03.602440Z",
     "shell.execute_reply.started": "2025-12-26T03:02:03.579978Z",
     "shell.execute_reply": "2025-12-26T03:02:03.601889Z"
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}