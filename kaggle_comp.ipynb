{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# MABe Mouse Behavior Detection - Inference Notebook\n",
    "\n",
    "This notebook loads a pre-trained MS-TCN++ model and generates predictions on the test set in submission format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Check if running on Kaggle\n",
    "ON_KAGGLE = os.path.exists('/kaggle/input')\n",
    "\n",
    "if ON_KAGGLE:\n",
    "    # Kaggle paths\n",
    "    DATA_DIR = Path('/kaggle/input/mabe-mouse-behavior-detection')\n",
    "    CHECKPOINT_PATH = Path('/kaggle/input/mabe-checkpoint/mabe-epoch=227-val_f1=0.4137.ckpt')\n",
    "    OUTPUT_DIR = Path('/kaggle/working')\n",
    "    # Add src to path\n",
    "    sys.path.insert(0, '/kaggle/input/mabe-src/src')\n",
    "else:\n",
    "    # Local development paths\n",
    "    DATA_DIR = Path('./kaggle/input/MABe-mouse-behavior-detection')\n",
    "    CHECKPOINT_PATH = Path('./outputs/checkpoints/mabe-epoch=227-val_f1=0.4137.ckpt')\n",
    "    OUTPUT_DIR = Path('./outputs')\n",
    "    sys.path.insert(0, str(Path('.').resolve()))\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Checkpoint path: {CHECKPOINT_PATH}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list-files",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available files in input directory\n",
    "if ON_KAGGLE:\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames[:10]:  # Limit output\n",
    "            print(os.path.join(dirname, filename))\n",
    "        if len(filenames) > 10:\n",
    "            print(f\"  ... and {len(filenames) - 10} more files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-src",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from src\n",
    "from src.models.lightning_module import BehaviorRecognitionModule\n",
    "from src.data.preprocessing import (\n",
    "    CoordinateNormalizer,\n",
    "    TemporalResampler,\n",
    "    MissingDataHandler,\n",
    "    BodyPartMapper\n",
    ")\n",
    "from src.utils.postprocessing import (\n",
    "    aggregate_window_predictions,\n",
    "    extract_segments,\n",
    "    merge_segments,\n",
    "    apply_nms,\n",
    "    create_submission,\n",
    "    BehaviorSegment,\n",
    ")\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and data configuration\n",
    "CONFIG = {\n",
    "    'window_size': 512,\n",
    "    'stride': 256,\n",
    "    'target_fps': 30.0,\n",
    "    'batch_size': 64,\n",
    "    'num_workers': 2,\n",
    "    \n",
    "    # Evaluation settings\n",
    "    'threshold': 0.39,\n",
    "    'min_duration': 5,\n",
    "    'smoothing_kernel': 5,\n",
    "    'nms_threshold': 0.3,\n",
    "    'merge_gap': 5,\n",
    "}\n",
    "\n",
    "# Behavior classes (must match training)\n",
    "BEHAVIORS = [\n",
    "    # Self behaviors\n",
    "    'biteobject', 'climb', 'dig', 'exploreobject', 'freeze',\n",
    "    'genitalgroom', 'huddle', 'rear', 'rest', 'run', 'selfgroom',\n",
    "    # Pair behaviors\n",
    "    'allogroom', 'approach', 'attack', 'attemptmount', 'avoid',\n",
    "    'chase', 'chaseattack', 'defend', 'disengage', 'dominance',\n",
    "    'dominancegroom', 'dominancemount', 'ejaculate', 'escape',\n",
    "    'flinch', 'follow', 'intromit', 'mount', 'reciprocalsniff',\n",
    "    'shepherd', 'sniff', 'sniffbody', 'sniffface', 'sniffgenital',\n",
    "    'submit', 'tussle'\n",
    "]\n",
    "\n",
    "print(f\"Number of behavior classes: {len(BEHAVIORS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-header",
   "metadata": {},
   "source": [
    "## Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for loading test tracking data (no annotations).\n",
    "    Generates sliding windows over all agent-target pairs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        metadata_df: pd.DataFrame,\n",
    "        tracking_dir: Path,\n",
    "        behaviors: List[str],\n",
    "        window_size: int = 512,\n",
    "        stride: int = 256,\n",
    "        target_fps: float = 30.0,\n",
    "        tracking_cache_size: int = 4\n",
    "    ):\n",
    "        self.tracking_dir = Path(tracking_dir)\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.target_fps = target_fps\n",
    "        self.tracking_cache_size = max(1, tracking_cache_size)\n",
    "        \n",
    "        self.behaviors = behaviors\n",
    "        self.num_classes = len(behaviors)\n",
    "        \n",
    "        # Preprocessors\n",
    "        self.coord_normalizer = CoordinateNormalizer()\n",
    "        self.temporal_resampler = TemporalResampler(target_fps)\n",
    "        self.missing_handler = MissingDataHandler()\n",
    "        self.bodypart_mapper = BodyPartMapper(use_core_only=False)\n",
    "        \n",
    "        # Cache\n",
    "        self._tracking_cache: OrderedDict = OrderedDict()\n",
    "        \n",
    "        # Parse metadata\n",
    "        self.metadata_df = metadata_df.copy()\n",
    "        self.video_ids = metadata_df['video_id'].unique().tolist()\n",
    "        \n",
    "        # Build sample index\n",
    "        self.samples = self._build_sample_index()\n",
    "        print(f\"Built {len(self.samples)} test samples from {len(self.video_ids)} videos\")\n",
    "    \n",
    "    def _get_fps(self, metadata: Dict) -> float:\n",
    "        return metadata.get('frames_per_second',\n",
    "                           metadata.get('frames per second',\n",
    "                                        metadata.get('fps', 30)))\n",
    "    \n",
    "    def _build_sample_index(self) -> List[Dict]:\n",
    "        samples = []\n",
    "        \n",
    "        for _, row in tqdm(self.metadata_df.iterrows(), total=len(self.metadata_df), desc=\"Building sample index\"):\n",
    "            lab_id = row['lab_id']\n",
    "            video_id = row['video_id']\n",
    "            fps = self._get_fps(row)\n",
    "            \n",
    "            # Load tracking to get video length and mice\n",
    "            track_path = self.tracking_dir / f\"{lab_id}\" / f\"{video_id}.parquet\"\n",
    "            if not track_path.exists():\n",
    "                continue\n",
    "            \n",
    "            track_df = pd.read_parquet(track_path)\n",
    "            n_frames = track_df['video_frame'].max() + 1\n",
    "            mice = sorted(track_df['mouse_id'].unique())\n",
    "            \n",
    "            # Adjust for resampling\n",
    "            if fps != self.target_fps:\n",
    "                duration = n_frames / fps\n",
    "                n_frames = int(duration * self.target_fps)\n",
    "            \n",
    "            # Generate windows for each agent-target pair\n",
    "            for start in range(0, max(1, n_frames - self.window_size + 1), self.stride):\n",
    "                for agent in mice:\n",
    "                    for target in mice:\n",
    "                        samples.append({\n",
    "                            'lab_id': lab_id,\n",
    "                            'video_id': video_id,\n",
    "                            'start_frame': start,\n",
    "                            'agent_id': agent,\n",
    "                            'target_id': target,\n",
    "                            'metadata': row.to_dict()\n",
    "                        })\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        sample_info = self.samples[idx]\n",
    "        features, valid_mask = self._load_tracking(sample_info)\n",
    "        \n",
    "        # Dummy labels (zeros) for test data\n",
    "        labels = np.zeros((self.window_size, self.num_classes), dtype=np.float32)\n",
    "        \n",
    "        return {\n",
    "            'features': torch.from_numpy(features),\n",
    "            'labels': torch.from_numpy(labels),\n",
    "            'valid_mask': torch.from_numpy(valid_mask),\n",
    "            'video_id': sample_info['video_id'],\n",
    "            'agent_id': sample_info['agent_id'],\n",
    "            'target_id': sample_info['target_id'],\n",
    "            'start_frame': sample_info['start_frame']\n",
    "        }\n",
    "    \n",
    "    def _get_cached_tracking(self, lab_id: str, video_id: str, metadata: Dict):\n",
    "        key = (lab_id, video_id)\n",
    "        if key in self._tracking_cache:\n",
    "            self._tracking_cache.move_to_end(key)\n",
    "            return self._tracking_cache[key]\n",
    "        \n",
    "        track_path = self.tracking_dir / f\"{lab_id}\" / f\"{video_id}.parquet\"\n",
    "        if not track_path.exists():\n",
    "            raise FileNotFoundError(f\"Tracking file not found: {track_path}\")\n",
    "        \n",
    "        track_df = pd.read_parquet(track_path)\n",
    "        fps = self._get_fps(metadata)\n",
    "        bodyparts = sorted(track_df['bodypart'].unique().tolist())\n",
    "        \n",
    "        coords_by_mouse = {}\n",
    "        valid_by_mouse = {}\n",
    "        \n",
    "        for mouse_id in track_df['mouse_id'].unique():\n",
    "            raw_coords = self._extract_mouse_coords(track_df, mouse_id, bodyparts)\n",
    "            mapped_coords, mapped_parts, availability = self.bodypart_mapper.map_bodyparts(raw_coords, bodyparts)\n",
    "            mapped_coords = self.bodypart_mapper.compute_derived_parts(mapped_coords, mapped_parts, availability)\n",
    "            \n",
    "            if fps != self.target_fps:\n",
    "                mapped_coords = self.temporal_resampler(mapped_coords, fps)\n",
    "            \n",
    "            mapped_coords = self.coord_normalizer(mapped_coords, metadata)\n",
    "            mapped_coords, valid_mask = self.missing_handler.interpolate_missing(mapped_coords)\n",
    "            mapped_coords = np.nan_to_num(mapped_coords, nan=0.0)\n",
    "            \n",
    "            coords_by_mouse[mouse_id] = mapped_coords.astype(np.float32)\n",
    "            valid_by_mouse[mouse_id] = valid_mask.astype(np.float32)\n",
    "        \n",
    "        cache_entry = {\n",
    "            'coords_by_mouse': coords_by_mouse,\n",
    "            'valid_by_mouse': valid_by_mouse\n",
    "        }\n",
    "        self._tracking_cache[key] = cache_entry\n",
    "        \n",
    "        if len(self._tracking_cache) > self.tracking_cache_size:\n",
    "            self._tracking_cache.popitem(last=False)\n",
    "        \n",
    "        return cache_entry\n",
    "    \n",
    "    def _extract_mouse_coords(self, track_df: pd.DataFrame, mouse_id: int, bodyparts: List[str]) -> Dict[str, np.ndarray]:\n",
    "        mouse_df = track_df[track_df['mouse_id'] == mouse_id].copy()\n",
    "        n_frames = track_df['video_frame'].max() + 1\n",
    "        \n",
    "        coords = {}\n",
    "        for bp in bodyparts:\n",
    "            bp_df = mouse_df[mouse_df['bodypart'] == bp].sort_values('video_frame')\n",
    "            frames = bp_df['video_frame'].values\n",
    "            part_coords = np.full((n_frames, 2), np.nan, dtype=np.float32)\n",
    "            part_coords[frames, 0] = bp_df['x'].values\n",
    "            part_coords[frames, 1] = bp_df['y'].values\n",
    "            coords[bp] = part_coords\n",
    "        \n",
    "        return coords\n",
    "    \n",
    "    def _load_tracking(self, sample_info: Dict) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        lab_id = sample_info['lab_id']\n",
    "        video_id = sample_info['video_id']\n",
    "        start_frame = sample_info['start_frame']\n",
    "        agent_id = sample_info['agent_id']\n",
    "        target_id = sample_info['target_id']\n",
    "        metadata = sample_info['metadata']\n",
    "        \n",
    "        cache_entry = self._get_cached_tracking(lab_id, video_id, metadata)\n",
    "        coords_by_mouse = cache_entry['coords_by_mouse']\n",
    "        valid_by_mouse = cache_entry['valid_by_mouse']\n",
    "        \n",
    "        agent_coords = coords_by_mouse.get(agent_id)\n",
    "        target_coords = coords_by_mouse.get(target_id)\n",
    "        agent_valid = valid_by_mouse.get(agent_id)\n",
    "        target_valid = valid_by_mouse.get(target_id)\n",
    "        \n",
    "        if agent_coords is None or target_coords is None:\n",
    "            raise ValueError(f\"Missing coordinates for agent {agent_id} or target {target_id} in {video_id}\")\n",
    "        \n",
    "        end_frame = start_frame + self.window_size\n",
    "        agent_window = self._get_window(agent_coords, start_frame, end_frame)\n",
    "        target_window = self._get_window(target_coords, start_frame, end_frame)\n",
    "        \n",
    "        features = np.concatenate([\n",
    "            agent_window.reshape(self.window_size, -1),\n",
    "            target_window.reshape(self.window_size, -1)\n",
    "        ], axis=-1)\n",
    "        \n",
    "        agent_valid_window = self._get_window(agent_valid.astype(np.float32), start_frame, end_frame)\n",
    "        target_valid_window = self._get_window(target_valid.astype(np.float32), start_frame, end_frame)\n",
    "        valid_mask = (agent_valid_window.mean(axis=-1) > 0.5) & (target_valid_window.mean(axis=-1) > 0.5)\n",
    "        \n",
    "        return features.astype(np.float32), valid_mask.astype(np.float32)\n",
    "    \n",
    "    def _get_window(self, data: np.ndarray, start: int, end: int) -> np.ndarray:\n",
    "        n_frames = data.shape[0]\n",
    "        \n",
    "        if start < 0:\n",
    "            pre_pad = -start\n",
    "            start = 0\n",
    "        else:\n",
    "            pre_pad = 0\n",
    "        \n",
    "        if end > n_frames:\n",
    "            post_pad = end - n_frames\n",
    "            end = n_frames\n",
    "        else:\n",
    "            post_pad = 0\n",
    "        \n",
    "        window = data[start:end]\n",
    "        \n",
    "        if pre_pad > 0 or post_pad > 0:\n",
    "            pad_width = [(pre_pad, post_pad)] + [(0, 0)] * (window.ndim - 1)\n",
    "            window = np.pad(window, pad_width, mode='edge')\n",
    "        \n",
    "        return window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load checkpoint with safe globals for numpy scalars\n",
    "safe_classes = [np.core.multiarray.scalar]\n",
    "try:\n",
    "    torch.serialization.add_safe_globals(safe_classes)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    safe_ctx = torch.serialization.safe_globals(safe_classes)\n",
    "except Exception:\n",
    "    from contextlib import nullcontext\n",
    "    safe_ctx = nullcontext()\n",
    "\n",
    "print(f\"Loading model from {CHECKPOINT_PATH}\")\n",
    "with safe_ctx:\n",
    "    model = BehaviorRecognitionModule.load_from_checkpoint(\n",
    "        str(CHECKPOINT_PATH),\n",
    "        map_location=device,\n",
    "        weights_only=False,\n",
    "        strict=True,\n",
    "    )\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"  - Model type: {model.model_name}\")\n",
    "print(f\"  - Input dim: {model.input_dim}\")\n",
    "print(f\"  - Num classes: {model.num_classes}\")\n",
    "print(f\"  - Behaviors: {model.behaviors[:5]}... ({len(model.behaviors)} total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test metadata\n",
    "test_csv = DATA_DIR / 'test.csv'\n",
    "test_df = pd.read_csv(test_csv)\n",
    "print(f\"Test metadata: {len(test_df)} videos\")\n",
    "print(test_df.head())\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = TestDataset(\n",
    "    metadata_df=test_df,\n",
    "    tracking_dir=DATA_DIR / 'test_tracking',\n",
    "    behaviors=BEHAVIORS,\n",
    "    window_size=CONFIG['window_size'],\n",
    "    stride=CONFIG['stride'],\n",
    "    target_fps=CONFIG['target_fps'],\n",
    "    tracking_cache_size=8\n",
    ")\n",
    "\n",
    "print(f\"\\nTest dataset: {len(test_dataset)} samples\")\n",
    "\n",
    "# Create dataloader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference-header",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_scalar(value: Any) -> Any:\n",
    "    \"\"\"Convert tensors/NumPy scalars to plain Python values.\"\"\"\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        value = value.item()\n",
    "    if isinstance(value, np.generic):\n",
    "        value = value.item()\n",
    "    return value\n",
    "\n",
    "\n",
    "def _format_mouse_id(mouse_id: Any, allow_self: bool = True) -> str:\n",
    "    \"\"\"Normalize mouse identifiers to submission format (mouseX or self).\"\"\"\n",
    "    mouse_id = _to_scalar(mouse_id)\n",
    "    if isinstance(mouse_id, str):\n",
    "        cleaned = mouse_id.strip()\n",
    "        if cleaned.lower().startswith('mouse'):\n",
    "            return cleaned\n",
    "        if cleaned.lstrip('-').isdigit():\n",
    "            mouse_id = int(cleaned)\n",
    "        else:\n",
    "            return cleaned\n",
    "    try:\n",
    "        mouse_int = int(mouse_id)\n",
    "    except (TypeError, ValueError):\n",
    "        return str(mouse_id)\n",
    "    \n",
    "    if allow_self and mouse_int == -1:\n",
    "        return 'self'\n",
    "    return f'mouse{mouse_int}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_predictions(\n",
    "    model: BehaviorRecognitionModule,\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Run model on dataloader and collect window-level predictions.\n",
    "    \"\"\"\n",
    "    window_predictions = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Running inference\"):\n",
    "            features = batch['features'].to(device)\n",
    "            mask = batch.get('valid_mask')\n",
    "            if mask is not None:\n",
    "                mask = mask.to(device)\n",
    "            \n",
    "            logits = model(features, mask)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            \n",
    "            for i in range(features.shape[0]):\n",
    "                window_prob = probs[i].detach().cpu().numpy()\n",
    "                if mask is not None:\n",
    "                    window_mask = mask[i].detach().cpu().numpy().reshape(-1, 1)\n",
    "                    window_prob = window_prob * window_mask\n",
    "                \n",
    "                window_predictions.append({\n",
    "                    'video_id': _to_scalar(batch['video_id'][i]),\n",
    "                    'agent_id': _to_scalar(batch['agent_id'][i]),\n",
    "                    'target_id': _to_scalar(batch['target_id'][i]),\n",
    "                    'start_frame': int(_to_scalar(batch['start_frame'][i])),\n",
    "                    'probabilities': window_prob,\n",
    "                })\n",
    "    \n",
    "    return window_predictions\n",
    "\n",
    "\n",
    "# Run inference\n",
    "print(\"Running inference on test data...\")\n",
    "window_predictions = collect_predictions(model, test_loader, device)\n",
    "print(f\"Collected {len(window_predictions)} window predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postprocess-header",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postprocess",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_submission_rows(\n",
    "    window_predictions: List[Dict],\n",
    "    behaviors: List[str],\n",
    "    threshold: float,\n",
    "    min_duration: int,\n",
    "    smoothing_kernel: int,\n",
    "    nms_threshold: float,\n",
    "    merge_gap: int = 5,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Convert window-level predictions into submission-format rows.\n",
    "    \"\"\"\n",
    "    print(\"Aggregating window predictions...\")\n",
    "    aggregated = aggregate_window_predictions(window_predictions, overlap_strategy='average')\n",
    "    print(f\"  {len(aggregated)} unique (video, agent, target) combinations\")\n",
    "    \n",
    "    all_segments: Dict[Tuple[int, Any, Any], List[BehaviorSegment]] = {}\n",
    "    \n",
    "    print(\"Extracting segments...\")\n",
    "    for (video_id, agent_id, target_id), frame_probs in tqdm(aggregated.items(), desc=\"Processing videos\"):\n",
    "        raw_segments = extract_segments(\n",
    "            frame_probs,\n",
    "            behaviors,\n",
    "            threshold=threshold,\n",
    "            min_duration=min_duration,\n",
    "            smoothing_kernel=smoothing_kernel,\n",
    "        )\n",
    "        merged = merge_segments(raw_segments, gap_threshold=merge_gap)\n",
    "        final_segments = apply_nms(merged, iou_threshold=nms_threshold)\n",
    "        \n",
    "        segment_objects = [\n",
    "            BehaviorSegment(\n",
    "                video_id=int(_to_scalar(video_id)),\n",
    "                agent_id=_format_mouse_id(agent_id, allow_self=False),\n",
    "                target_id=_format_mouse_id(target_id, allow_self=True),\n",
    "                action=behavior,\n",
    "                start_frame=int(start),\n",
    "                stop_frame=int(stop),\n",
    "                confidence=float(conf),\n",
    "            )\n",
    "            for behavior, start, stop, conf in final_segments\n",
    "        ]\n",
    "        all_segments[(video_id, agent_id, target_id)] = segment_objects\n",
    "    \n",
    "    print(\"Creating submission rows...\")\n",
    "    return create_submission(all_segments, min_duration=min_duration)\n",
    "\n",
    "\n",
    "# Build submission\n",
    "submission_rows = build_submission_rows(\n",
    "    window_predictions,\n",
    "    BEHAVIORS,\n",
    "    threshold=CONFIG['threshold'],\n",
    "    min_duration=CONFIG['min_duration'],\n",
    "    smoothing_kernel=CONFIG['smoothing_kernel'],\n",
    "    nms_threshold=CONFIG['nms_threshold'],\n",
    "    merge_gap=CONFIG['merge_gap'],\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(submission_rows)} submission rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "submission-header",
   "metadata": {},
   "source": [
    "## Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame(\n",
    "    submission_rows,\n",
    "    columns=['row_id', 'video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame']\n",
    ")\n",
    "\n",
    "# Save submission\n",
    "submission_path = OUTPUT_DIR / 'submission.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission saved to {submission_path}\")\n",
    "print(f\"\\nSubmission shape: {submission_df.shape}\")\n",
    "print(f\"\\nSubmission preview:\")\n",
    "print(submission_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "submission-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission statistics\n",
    "print(\"\\n=== Submission Statistics ===\")\n",
    "print(f\"Total predictions: {len(submission_df)}\")\n",
    "print(f\"Unique videos: {submission_df['video_id'].nunique()}\")\n",
    "print(f\"\\nPredictions per action:\")\n",
    "print(submission_df['action'].value_counts().head(20))\n",
    "\n",
    "print(f\"\\nAverage segment duration:\")\n",
    "submission_df['duration'] = submission_df['stop_frame'] - submission_df['start_frame']\n",
    "print(f\"  Mean: {submission_df['duration'].mean():.1f} frames\")\n",
    "print(f\"  Median: {submission_df['duration'].median():.1f} frames\")\n",
    "print(f\"  Min: {submission_df['duration'].min()} frames\")\n",
    "print(f\"  Max: {submission_df['duration'].max()} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check\n",
    "print(\"\\n=== Final Checks ===\")\n",
    "print(f\"Submission file exists: {submission_path.exists()}\")\n",
    "print(f\"File size: {submission_path.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Verify columns\n",
    "expected_columns = ['row_id', 'video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame']\n",
    "actual_columns = list(submission_df.columns[:7])\n",
    "print(f\"\\nColumns match expected: {actual_columns == expected_columns}\")\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
