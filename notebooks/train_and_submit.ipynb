{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MABe-2.0 Behavior Recognition: MS-TCN++ Training and Inference\n",
    "\n",
    "This notebook implements a multi-stage temporal convolutional network (MS-TCN++) for multi-agent behavior recognition in the MABe Challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (for Kaggle)\n",
    "!pip install pytorch-lightning omegaconf rich -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from scipy.ndimage import median_filter, uniform_filter1d\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set random seed\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'data_dir': '/kaggle/input/MABe-mouse-behavior-detection',\n",
    "    'output_dir': '/kaggle/working',\n",
    "    'window_size': 256,\n",
    "    'stride': 128,\n",
    "    'batch_size': 8,\n",
    "    'num_epochs': 50,\n",
    "    'learning_rate': 0.0005,\n",
    "    'num_stages': 4,\n",
    "    'num_layers': 8,\n",
    "    'num_f_maps': 64,\n",
    "    'dropout': 0.5,\n",
    "    'threshold': 0.5,\n",
    "    'min_duration': 5\n",
    "}\n",
    "\n",
    "# Kaggle paths\n",
    "INPUT_DIR = Path(CONFIG['data_dir'])\n",
    "OUTPUT_DIR = Path(CONFIG['output_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MABeDataset(Dataset):\n",
    "    \"\"\"Dataset for MABe behavior recognition.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        metadata_df: pd.DataFrame,\n",
    "        tracking_dir: Path,\n",
    "        annotation_dir: Optional[Path] = None,\n",
    "        behaviors: Optional[List[str]] = None,\n",
    "        window_size: int = 256,\n",
    "        stride: int = 128,\n",
    "        is_train: bool = True\n",
    "    ):\n",
    "        self.tracking_dir = Path(tracking_dir)\n",
    "        self.annotation_dir = Path(annotation_dir) if annotation_dir else None\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.is_train = is_train\n",
    "        self.metadata_df = metadata_df\n",
    "        \n",
    "        # Build behavior vocabulary\n",
    "        if behaviors is None:\n",
    "            self.behaviors = self._collect_behaviors()\n",
    "        else:\n",
    "            self.behaviors = behaviors\n",
    "        self.behavior_to_idx = {b: i for i, b in enumerate(self.behaviors)}\n",
    "        self.num_classes = len(self.behaviors)\n",
    "        \n",
    "        # Build sample index\n",
    "        self.samples = self._build_samples()\n",
    "        \n",
    "    def _collect_behaviors(self) -> List[str]:\n",
    "        \"\"\"Collect unique behaviors from annotations.\"\"\"\n",
    "        if self.annotation_dir is None:\n",
    "            return []\n",
    "        behaviors = set()\n",
    "        for _, row in self.metadata_df.iterrows():\n",
    "            ann_path = self.annotation_dir / row['lab_id'] / f\"{row['video_id']}.parquet\"\n",
    "            if ann_path.exists():\n",
    "                ann_df = pd.read_parquet(ann_path)\n",
    "                behaviors.update(ann_df['action'].unique())\n",
    "        return sorted(list(behaviors))\n",
    "    \n",
    "    def _build_samples(self) -> List[Dict]:\n",
    "        \"\"\"Build sample index.\"\"\"\n",
    "        samples = []\n",
    "        for _, row in self.metadata_df.iterrows():\n",
    "            lab_id = row['lab_id']\n",
    "            video_id = row['video_id']\n",
    "            \n",
    "            track_path = self.tracking_dir / lab_id / f\"{video_id}.parquet\"\n",
    "            if not track_path.exists():\n",
    "                continue\n",
    "                \n",
    "            track_df = pd.read_parquet(track_path)\n",
    "            n_frames = track_df['video_frame'].max() + 1\n",
    "            mice = track_df['mouse_id'].unique()\n",
    "            \n",
    "            for start in range(0, max(1, n_frames - self.window_size + 1), self.stride):\n",
    "                for agent in mice:\n",
    "                    for target in mice:\n",
    "                        samples.append({\n",
    "                            'lab_id': lab_id,\n",
    "                            'video_id': video_id,\n",
    "                            'start_frame': start,\n",
    "                            'agent_id': agent,\n",
    "                            'target_id': target,\n",
    "                            'metadata': row.to_dict()\n",
    "                        })\n",
    "        return samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Load tracking data\n",
    "        features = self._load_features(sample)\n",
    "        \n",
    "        # Load labels if available\n",
    "        if self.annotation_dir:\n",
    "            labels = self._load_labels(sample)\n",
    "        else:\n",
    "            labels = np.zeros((self.window_size, self.num_classes), dtype=np.float32)\n",
    "        \n",
    "        return {\n",
    "            'features': torch.from_numpy(features),\n",
    "            'labels': torch.from_numpy(labels),\n",
    "            'video_id': sample['video_id'],\n",
    "            'agent_id': sample['agent_id'],\n",
    "            'target_id': sample['target_id'],\n",
    "            'start_frame': sample['start_frame']\n",
    "        }\n",
    "    \n",
    "    def _load_features(self, sample) -> np.ndarray:\n",
    "        \"\"\"Load and preprocess tracking features.\"\"\"\n",
    "        track_path = self.tracking_dir / sample['lab_id'] / f\"{sample['video_id']}.parquet\"\n",
    "        track_df = pd.read_parquet(track_path)\n",
    "        \n",
    "        # Extract coordinates for agent and target\n",
    "        agent_coords = self._extract_coords(track_df, sample['agent_id'])\n",
    "        target_coords = self._extract_coords(track_df, sample['target_id'])\n",
    "        \n",
    "        # Get window\n",
    "        start = sample['start_frame']\n",
    "        end = start + self.window_size\n",
    "        \n",
    "        agent_window = self._get_window(agent_coords, start, end)\n",
    "        target_window = self._get_window(target_coords, start, end)\n",
    "        \n",
    "        # Normalize\n",
    "        pix_per_cm = sample['metadata'].get('pix per cm (approx)', 1.0)\n",
    "        if pix_per_cm and pix_per_cm > 0:\n",
    "            agent_window = agent_window / pix_per_cm\n",
    "            target_window = target_window / pix_per_cm\n",
    "        \n",
    "        # Flatten and concatenate\n",
    "        features = np.concatenate([\n",
    "            agent_window.reshape(self.window_size, -1),\n",
    "            target_window.reshape(self.window_size, -1)\n",
    "        ], axis=-1)\n",
    "        \n",
    "        # Handle NaN\n",
    "        features = np.nan_to_num(features, nan=0.0)\n",
    "        \n",
    "        return features.astype(np.float32)\n",
    "    \n",
    "    def _extract_coords(self, track_df, mouse_id) -> np.ndarray:\n",
    "        \"\"\"Extract coordinates for a mouse.\"\"\"\n",
    "        mouse_df = track_df[track_df['mouse_id'] == mouse_id]\n",
    "        n_frames = track_df['video_frame'].max() + 1\n",
    "        bodyparts = mouse_df['bodypart'].unique()\n",
    "        \n",
    "        coords = np.full((n_frames, len(bodyparts), 2), np.nan, dtype=np.float32)\n",
    "        \n",
    "        for i, bp in enumerate(bodyparts):\n",
    "            bp_df = mouse_df[mouse_df['bodypart'] == bp].sort_values('video_frame')\n",
    "            frames = bp_df['video_frame'].values\n",
    "            coords[frames, i, 0] = bp_df['x'].values\n",
    "            coords[frames, i, 1] = bp_df['y'].values\n",
    "        \n",
    "        return coords\n",
    "    \n",
    "    def _get_window(self, data, start, end) -> np.ndarray:\n",
    "        \"\"\"Extract window with padding.\"\"\"\n",
    "        n_frames = data.shape[0]\n",
    "        \n",
    "        if start < 0:\n",
    "            pre_pad = -start\n",
    "            start = 0\n",
    "        else:\n",
    "            pre_pad = 0\n",
    "            \n",
    "        if end > n_frames:\n",
    "            post_pad = end - n_frames\n",
    "            end = n_frames\n",
    "        else:\n",
    "            post_pad = 0\n",
    "        \n",
    "        window = data[start:end]\n",
    "        \n",
    "        if pre_pad > 0 or post_pad > 0:\n",
    "            pad_width = [(pre_pad, post_pad)] + [(0, 0)] * (window.ndim - 1)\n",
    "            window = np.pad(window, pad_width, mode='edge')\n",
    "        \n",
    "        return window\n",
    "    \n",
    "    def _load_labels(self, sample) -> np.ndarray:\n",
    "        \"\"\"Load annotation labels.\"\"\"\n",
    "        labels = np.zeros((self.window_size, self.num_classes), dtype=np.float32)\n",
    "        \n",
    "        ann_path = self.annotation_dir / sample['lab_id'] / f\"{sample['video_id']}.parquet\"\n",
    "        if not ann_path.exists():\n",
    "            return labels\n",
    "        \n",
    "        ann_df = pd.read_parquet(ann_path)\n",
    "        \n",
    "        # Filter for this pair\n",
    "        pair_anns = ann_df[\n",
    "            (ann_df['agent_id'] == f\"mouse{sample['agent_id']}\") &\n",
    "            ((ann_df['target_id'] == f\"mouse{sample['target_id']}\") | \n",
    "             (ann_df['target_id'] == 'self'))\n",
    "        ]\n",
    "        \n",
    "        start = sample['start_frame']\n",
    "        \n",
    "        for _, row in pair_anns.iterrows():\n",
    "            action = row['action']\n",
    "            if action not in self.behavior_to_idx:\n",
    "                continue\n",
    "            \n",
    "            action_idx = self.behavior_to_idx[action]\n",
    "            window_start = max(0, row['start_frame'] - start)\n",
    "            window_end = min(self.window_size, row['stop_frame'] - start)\n",
    "            \n",
    "            if window_start < window_end:\n",
    "                labels[window_start:window_end, action_idx] = 1.0\n",
    "        \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MS-TCN++ Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DilatedResidualLayer(nn.Module):\n",
    "    \"\"\"Dilated residual layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, dilation, in_channels, out_channels, kernel_size=3, dropout=0.3):\n",
    "        super().__init__()\n",
    "        padding = (kernel_size - 1) * dilation // 2\n",
    "        \n",
    "        self.conv_dilated = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                                       padding=padding, dilation=dilation)\n",
    "        self.conv_1x1 = nn.Conv1d(out_channels, out_channels, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.skip = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv_dilated(x))\n",
    "        out = self.conv_1x1(out)\n",
    "        out = self.dropout(out)\n",
    "        return out + self.skip(x)\n",
    "\n",
    "\n",
    "class SingleStageTCN(nn.Module):\n",
    "    \"\"\"Single stage TCN.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, num_layers, num_f_maps, num_classes, kernel_size=3, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv1d(in_channels, num_f_maps, 1)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DilatedResidualLayer(2**i, num_f_maps, num_f_maps, kernel_size, dropout)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.conv_out = nn.Conv1d(num_f_maps, num_classes, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv_in(x)\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        return self.conv_out(out)\n",
    "\n",
    "\n",
    "class RefinementStage(nn.Module):\n",
    "    \"\"\"Refinement stage.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers, num_f_maps, num_classes, kernel_size=3, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv1d(num_classes, num_f_maps, 1)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DilatedResidualLayer(2**i, num_f_maps, num_f_maps, kernel_size, dropout)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.conv_out = nn.Conv1d(num_f_maps, num_classes, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv_in(x)\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        return self.conv_out(out)\n",
    "\n",
    "\n",
    "class MSTCN(nn.Module):\n",
    "    \"\"\"Multi-Stage TCN.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, num_classes, num_stages=4, num_layers=10, \n",
    "                 num_f_maps=64, kernel_size=3, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stage1 = SingleStageTCN(input_dim, num_layers, num_f_maps, num_classes, kernel_size, dropout)\n",
    "        self.stages = nn.ModuleList([\n",
    "            RefinementStage(num_layers, num_f_maps, num_classes, kernel_size, dropout)\n",
    "            for _ in range(num_stages - 1)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x = x.transpose(1, 2)  # (B, C, T)\n",
    "        \n",
    "        stage_outputs = []\n",
    "        out = self.stage1(x)\n",
    "        stage_outputs.append(out)\n",
    "        \n",
    "        for stage in self.stages:\n",
    "            out = stage(F.softmax(out, dim=1))\n",
    "            stage_outputs.append(out)\n",
    "        \n",
    "        final = out.transpose(1, 2)  # (B, T, C)\n",
    "        stage_outputs = [s.transpose(1, 2) for s in stage_outputs]\n",
    "        \n",
    "        return final, stage_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BehaviorModule(pl.LightningModule):\n",
    "    \"\"\"Lightning module for training.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, num_classes, behaviors, config):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.behaviors = behaviors\n",
    "        self.config = config\n",
    "        \n",
    "        self.model = MSTCN(\n",
    "            input_dim=input_dim,\n",
    "            num_classes=num_classes,\n",
    "            num_stages=config['num_stages'],\n",
    "            num_layers=config['num_layers'],\n",
    "            num_f_maps=config['num_f_maps'],\n",
    "            dropout=config['dropout']\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        predictions, _ = self.model(x, mask)\n",
    "        return predictions\n",
    "    \n",
    "    def _compute_loss(self, predictions, stage_outputs, labels):\n",
    "        # Multi-stage loss\n",
    "        total_loss = 0\n",
    "        for stage_out in stage_outputs:\n",
    "            loss = F.binary_cross_entropy_with_logits(stage_out, labels)\n",
    "            total_loss += loss\n",
    "        \n",
    "        # Smoothing loss\n",
    "        log_probs = F.log_softmax(predictions, dim=-1)\n",
    "        diff = log_probs[:, 1:] - log_probs[:, :-1]\n",
    "        smooth_loss = torch.clamp(diff ** 2, 0, 16).mean()\n",
    "        \n",
    "        return total_loss / len(stage_outputs) + 0.15 * smooth_loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        features = batch['features']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        predictions, stage_outputs = self.model(features)\n",
    "        loss = self._compute_loss(predictions, stage_outputs, labels)\n",
    "        \n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        features = batch['features']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        predictions, stage_outputs = self.model(features)\n",
    "        loss = self._compute_loss(predictions, stage_outputs, labels)\n",
    "        \n",
    "        # F1 score\n",
    "        probs = torch.sigmoid(predictions)\n",
    "        preds = (probs > 0.5).float()\n",
    "        \n",
    "        tp = (preds * labels).sum()\n",
    "        fp = (preds * (1 - labels)).sum()\n",
    "        fn = ((1 - preds) * labels).sum()\n",
    "        \n",
    "        precision = tp / (tp + fp + 1e-8)\n",
    "        recall = tp / (tp + fn + 1e-8)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "        \n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_f1', f1)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.config['learning_rate'],\n",
    "            weight_decay=0.0001\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=self.config['num_epochs']\n",
    "        )\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "train_df = pd.read_csv(INPUT_DIR / 'train.csv')\n",
    "print(f\"Training videos: {len(train_df)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MABeDataset(\n",
    "    metadata_df=train_df.iloc[:-10],  # Use most for training\n",
    "    tracking_dir=INPUT_DIR / 'train_tracking',\n",
    "    annotation_dir=INPUT_DIR / 'train_annotation',\n",
    "    window_size=CONFIG['window_size'],\n",
    "    stride=CONFIG['stride']\n",
    ")\n",
    "\n",
    "val_dataset = MABeDataset(\n",
    "    metadata_df=train_df.iloc[-10:],  # Last 10 for validation\n",
    "    tracking_dir=INPUT_DIR / 'train_tracking',\n",
    "    annotation_dir=INPUT_DIR / 'train_annotation',\n",
    "    behaviors=train_dataset.behaviors,\n",
    "    window_size=CONFIG['window_size'],\n",
    "    stride=CONFIG['window_size']\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Behaviors: {train_dataset.behaviors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Get input dimension from first sample\n",
    "sample = train_dataset[0]\n",
    "input_dim = sample['features'].shape[-1]\n",
    "print(f\"Input dimension: {input_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = BehaviorModule(\n",
    "    input_dim=input_dim,\n",
    "    num_classes=train_dataset.num_classes,\n",
    "    behaviors=train_dataset.behaviors,\n",
    "    config=CONFIG\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath=OUTPUT_DIR,\n",
    "        filename='best-{epoch:02d}-{val_f1:.4f}',\n",
    "        monitor='val_f1',\n",
    "        mode='max',\n",
    "        save_top_k=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_f1',\n",
    "        mode='max',\n",
    "        patience=10\n",
    "    )\n",
    "]\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=CONFIG['num_epochs'],\n",
    "    accelerator='auto',\n",
    "    callbacks=callbacks,\n",
    "    log_every_n_steps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_segments(frame_probs, behavior_names, threshold=0.5, min_duration=5):\n",
    "    \"\"\"Convert frame predictions to segments.\"\"\"\n",
    "    segments = []\n",
    "    n_frames, n_behaviors = frame_probs.shape\n",
    "    \n",
    "    for b_idx in range(n_behaviors):\n",
    "        probs = median_filter(frame_probs[:, b_idx], size=5)\n",
    "        binary = (probs >= threshold).astype(int)\n",
    "        \n",
    "        # Find contiguous regions\n",
    "        diff = np.diff(np.concatenate([[0], binary, [0]]))\n",
    "        starts = np.where(diff == 1)[0]\n",
    "        ends = np.where(diff == -1)[0]\n",
    "        \n",
    "        for start, end in zip(starts, ends):\n",
    "            if end - start >= min_duration:\n",
    "                segments.append({\n",
    "                    'action': behavior_names[b_idx],\n",
    "                    'start_frame': int(start),\n",
    "                    'stop_frame': int(end),\n",
    "                    'confidence': float(probs[start:end].mean())\n",
    "                })\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_df = pd.read_csv(INPUT_DIR / 'test.csv')\n",
    "\n",
    "test_dataset = MABeDataset(\n",
    "    metadata_df=test_df,\n",
    "    tracking_dir=INPUT_DIR / 'test_tracking',\n",
    "    annotation_dir=None,\n",
    "    behaviors=train_dataset.behaviors,\n",
    "    window_size=CONFIG['window_size'],\n",
    "    stride=CONFIG['window_size'] // 2\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "model.eval()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "all_predictions = defaultdict(list)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Running inference\"):\n",
    "        features = batch['features'].to(device)\n",
    "        predictions = model(features)\n",
    "        probs = torch.sigmoid(predictions).cpu().numpy()\n",
    "        \n",
    "        for i in range(len(batch['video_id'])):\n",
    "            key = (\n",
    "                batch['video_id'][i],\n",
    "                batch['agent_id'][i],\n",
    "                batch['target_id'][i]\n",
    "            )\n",
    "            all_predictions[key].append({\n",
    "                'start_frame': batch['start_frame'][i],\n",
    "                'probs': probs[i]\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate and extract segments\n",
    "submission_rows = []\n",
    "row_id = 0\n",
    "\n",
    "for (video_id, agent_id, target_id), preds in tqdm(all_predictions.items()):\n",
    "    # Find total length\n",
    "    max_frame = max(p['start_frame'] + p['probs'].shape[0] for p in preds)\n",
    "    n_classes = preds[0]['probs'].shape[-1]\n",
    "    \n",
    "    # Aggregate\n",
    "    sum_probs = np.zeros((max_frame, n_classes))\n",
    "    counts = np.zeros(max_frame)\n",
    "    \n",
    "    for p in preds:\n",
    "        start = p['start_frame']\n",
    "        end = start + p['probs'].shape[0]\n",
    "        sum_probs[start:end] += p['probs']\n",
    "        counts[start:end] += 1\n",
    "    \n",
    "    mask = counts > 0\n",
    "    sum_probs[mask] /= counts[mask, np.newaxis]\n",
    "    \n",
    "    # Extract segments\n",
    "    segments = extract_segments(\n",
    "        sum_probs,\n",
    "        train_dataset.behaviors,\n",
    "        threshold=CONFIG['threshold'],\n",
    "        min_duration=CONFIG['min_duration']\n",
    "    )\n",
    "    \n",
    "    for seg in segments:\n",
    "        submission_rows.append({\n",
    "            'row_id': row_id,\n",
    "            'video_id': video_id,\n",
    "            'agent_id': f\"mouse{agent_id}\" if isinstance(agent_id, int) else agent_id,\n",
    "            'target_id': f\"mouse{target_id}\" if isinstance(target_id, int) and target_id >= 0 else 'self',\n",
    "            'action': seg['action'],\n",
    "            'start_frame': seg['start_frame'],\n",
    "            'stop_frame': seg['stop_frame']\n",
    "        })\n",
    "        row_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "submission_df = pd.DataFrame(submission_rows)\n",
    "submission_df = submission_df.sort_values(['video_id', 'agent_id', 'target_id', 'start_frame'])\n",
    "submission_df.to_csv(OUTPUT_DIR / 'submission.csv', index=False)\n",
    "\n",
    "print(f\"Submission saved with {len(submission_df)} rows\")\n",
    "print(submission_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
